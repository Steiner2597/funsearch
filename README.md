# FunSearch-Lite

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Tests Passing](https://img.shields.io/badge/tests-passing-brightgreen.svg)](tests/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**FunSearch-Lite** is an evolutionary search system guided by Large Language Models (LLMs) for automated discovery and optimization of algorithmic heuristics. This project is a streamlined implementation of Google DeepMind's [FunSearch](https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/), specifically designed for AI course projects and research prototyping.

**FunSearch-Lite** æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è¿›åŒ–æœç´¢ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨å‘ç°å’Œä¼˜åŒ–ç®—æ³•å¯å‘å¼å‡½æ•°ã€‚æœ¬é¡¹ç›®æ˜¯å¯¹ Google DeepMind [FunSearch](https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/) è®ºæ–‡çš„ç²¾ç®€å®ç°ï¼Œä¸“ä¸º AI è¯¾ç¨‹é¡¹ç›®è®¾è®¡ã€‚

---

## ğŸš€ Key Innovations | æ ¸å¿ƒç‰¹æ€§

### 1. Multi-Fidelity Evaluation | å¤šä¿çœŸåº¦è¯„ä¼°
- **Cheap Eval**: Rapidly filter all candidates using a small set of test instances.
- **Full Eval**: Precisely evaluate only the Top-K candidates using the complete benchmark.
- **Impact**: Significantly reduces evaluation costs while maintaining search quality.

### 2. Multi-Model Collaboration | å¤šæ¨¡å‹åä½œ
- **Generator Model**: Use cost-effective models (e.g., DeepSeek-Chat, GPT-3.5) for bulk candidate generation.
- **Refiner Model**: Use powerful models (e.g., GPT-4) to optimize Top-K candidates.
- **Impact**: Balances search breadth with optimization depth.

### 3. Standard Benchmark Support | æ ‡å‡†åŸºå‡†æ”¯æŒ
- **OR-Library**: Built-in support for OR-Library Bin Packing instances (Falkenauer u* and t*).
- **Comparability**: Directly compare results with academic literature.

### 4. Search Trajectory Observability | æœç´¢å¯è§‚æµ‹æ€§
- **Real-time Progress**: `tqdm` integration showing generation progress and ETA.
- **Metric Tracking**: Best/average scores per generation and failure taxonomy.
- **Visualization**: Automated generation of evolution curves and failure distribution charts.

### 5. Sandbox Safe Execution | æ²™ç®±å®‰å…¨æ‰§è¡Œ
- **Process Isolation**: LLM-generated code runs in isolated sub-processes.
- **Resource Limits**: Timeout protection and import whitelisting (math, random, etc.).
- **Impact**: Prevents accidental or malicious code from compromising the host system.

---

## ğŸ› ï¸ Quick Start | å¿«é€Ÿå¼€å§‹

### Installation | å®‰è£…

```bash
# Install in editable mode
pip install -e .

# Or install dependencies manually
pip install pydantic pyyaml typer matplotlib openai tqdm
```

### Run Experiments | è¿è¡Œå®éªŒ

**Recommended: Using the `run.py` wrapper**

```bash
# 1. Set up API Key (once)
echo "DEEPSEEK_API_KEY=sk-xxx" > .env

# 2. Launch with one command
python run.py                     # Default (Random dataset)
python run.py -d orlib             # Use OR-Library dataset
python run.py -d orlib -s large    # OR-Library large instances
python run.py --demo               # Demo mode (No API Key required)
python run.py -g 3 -p 3 -y         # Quick test (~5 mins)
```

**Advanced: Using the CLI**

```bash
# Run with specific config and A/B variant
python -m experiments.cli run configs/binpacking_deepseek.yaml --variant a
```

---

## ğŸ—ï¸ System Architecture | ç³»ç»Ÿæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Experiment Runner                       â”‚
â”‚  (Config / Artifacts / Metrics / Plotting / Progress)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  FunSearchLoop   â”‚  â† Main Evolution Loop (tqdm)
          â”‚  + GlobalProgressâ”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚   LLM    â”‚  â”‚ Sandboxâ”‚  â”‚Evaluator â”‚
â”‚Providers â”‚  â”‚Executorâ”‚  â”‚(Multi-   â”‚
â”‚(DeepSeek/â”‚  â”‚(Safety)â”‚  â”‚Fidelity) â”‚
â”‚ OpenAI)  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚             â”‚
     â”‚            â”‚       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â””â”€â”€â”€â”€â”€â”€â–ºâ”‚ Candidate â”‚
â”‚LLM Cache â”‚              â”‚   Store     â”‚
â”‚(Cost Red)â”‚              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                          â”‚ OR-Libraryâ”‚
                          â”‚ Datasets  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure | é¡¹ç›®ç»“æ„

```text
funsearch/
â”œâ”€â”€ run.py                   # Unified entry point (Recommended)
â”œâ”€â”€ funsearch_core/          # Core evolutionary algorithm
â”‚   â”œâ”€â”€ loop.py              # Main loop with tqdm progress
â”‚   â”œâ”€â”€ population.py        # Population management
â”‚   â””â”€â”€ deduplication.py     # Functional deduplication (AST-based)
â”œâ”€â”€ evaluator/               # Problem-specific evaluation
â”‚   â”œâ”€â”€ bin_packing.py       # Bin Packing implementation
â”‚   â””â”€â”€ datasets.py          # OR-Library loader
â”œâ”€â”€ llm/                     # LLM integration layer
â”‚   â”œâ”€â”€ providers.py         # Multi-provider support (DeepSeek/OpenAI)
â”‚   â””â”€â”€ cache.py             # SQLite-based response caching
â”œâ”€â”€ sandbox/                 # Secure execution environment
â”‚   â”œâ”€â”€ executor.py          # Subprocess isolation
â”‚   â””â”€â”€ policy.py            # Resource & import limits
â”œâ”€â”€ experiments/             # Experiment orchestration
â”‚   â”œâ”€â”€ cli.py               # Typer-based CLI
â”‚   â”œâ”€â”€ metrics.py           # KPI collection
â”‚   â””â”€â”€ plotting.py          # Matplotlib visualizations
â”œâ”€â”€ artifacts/               # Experiment outputs (Auto-generated)
â”œâ”€â”€ configs/                 # YAML configuration files
â””â”€â”€ docs/                    # Detailed documentation
```

---

## ğŸ“Š A/B Testing & Metrics | å®éªŒä¸æŒ‡æ ‡

### A/B Testing
Use the `--variant` flag to compare different strategies (e.g., different prompts or selection logic):
```bash
python -m experiments.cli run configs/binpacking.yaml --variant a
python -m experiments.cli run configs/binpacking.yaml --variant b
```

### Report Generation
Generate comprehensive reports after runs complete:
```bash
python -m experiments.cli report artifacts/run_20250131_120000
python -m experiments.cli report artifacts/run_20250131_120000 --format html
```

Output includes:
- `report.md`: Markdown summary with key metrics and evolution analysis
- `report.html`: Self-contained HTML with embedded charts

See [docs/READING_REPORTS.md](docs/READING_REPORTS.md) for interpretation guidance.

### Run Comparison
Compare two runs (e.g., A/B variants):
```bash
python -m experiments.cli compare artifacts/run_A artifacts/run_B
python -m experiments.cli compare artifacts/run_A artifacts/run_B --output comparison.md
```

### Key Performance Indicators (KPIs)
- **Best Score**: The highest fitness achieved in the population.
- **Pass Rate**: Percentage of LLM-generated candidates that pass syntax and runtime checks.
- **Diversity Index**: Measure of functional uniqueness in the population.
- **Evaluation Efficiency**: Ratio of cheap vs. full evaluations.

---

## ğŸ“¦ Output Artifacts | å®éªŒäº§å‡º

Each run generates a dedicated folder in `artifacts/`:
- `config.yaml`: Snapshot of the experiment configuration.
- `candidates.db`: SQLite database containing all generated programs and scores.
- `metrics.jsonl`: Time-series data of all KPIs.
- `best_candidate.py`: The best performing heuristic function found.
- `plots/`: Evolution curves and failure distribution charts.

---

## ğŸ“ˆ Performance Benchmarks | æ€§èƒ½åŸºå‡†

| Dataset | Instances | Best Known | FunSearch-Lite | Gap | Time |
|---------|-----------|------------|----------------|-----|------|
| OR-Lib Small | 80 | 3921 (Total) | 3945 | +0.6% | 4h |
| OR-Lib Large | 80 | 12450 (Total) | 12580 | +1.0% | 8h |
| Random Small | 100 | N/A | -12.4 (Avg) | N/A | 1h |

*Note: Benchmarks are placeholders and vary based on LLM model and search budget.*

---

## ğŸ¤ Contributing | è´¡çŒ®æŒ‡å—

1. **Add New Tasks**: Implement `BaseEvaluator` in `evaluator/`.
2. **Improve LLM Prompts**: Modify templates in `llm/prompts.py`.
3. **Optimize Search**: Experiment with new selection or mutation strategies in `funsearch_core/`.

---

## ğŸ“ Academic Context | å­¦æœ¯èƒŒæ™¯

This project was developed as part of the AI Course at [Your University]. It demonstrates:
1. **Program Synthesis**: Using LLMs to write executable code.
2. **Evolutionary Computation**: Population-based search and diversity maintenance.
3. **System Observability**: Building transparent AI search processes.

**Attribution**: Based on the methodology described in *Romera-Paredes, B. et al. "Mathematical discoveries from program search with large language models." Nature (2023).*

---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.
