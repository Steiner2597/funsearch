# Metrics Schema & Explanation

This document details the metrics collected during FunSearch-Lite experiments and how to interpret them.

## Generation Metrics

For each generation in the evolutionary loop, a `GenerationMetrics` object is recorded.

| Field | Type | Description |
|-------|------|-------------|
| `generation` | `int` | The current generation number (starting from 0). |
| `best_score_cheap` | `float` | The highest score achieved during "Cheap Evaluation" (small test set). |
| `avg_score_cheap` | `float` | The average score of all valid candidates during "Cheap Evaluation". |
| `best_score_full` | `float` | The highest score achieved during "Full Evaluation" (complete benchmark). |
| `avg_score_full` | `float` | The average score of Top-K candidates during "Full Evaluation". |
| `n_generated` | `int` | Total number of candidates generated by the LLM in this generation. |
| `n_deduped` | `int` | Number of candidates removed due to functional duplication (AST-based). |
| `n_failed` | `int` | Number of candidates that failed syntax or runtime checks. |
| `failure_breakdown` | `dict` | Breakdown of failure types (e.g., `syntax`, `timeout`, `runtime`). |
| `eval_time_ms` | `float` | Total time spent on evaluation in milliseconds. |
| `timestamp` | `string` | ISO 8601 timestamp of when the generation completed. |

## Failure Taxonomy

The `failure_breakdown` dictionary categorizes why LLM-generated code failed to execute:

- **`syntax`**: The code is not valid Python (e.g., missing colons, indentation errors).
- **`timeout`**: The code exceeded the execution time limit in the sandbox.
- **`runtime`**: The code raised an exception during execution (e.g., `ZeroDivisionError`, `IndexError`).
- **`import_blocked`**: The code attempted to import a module not in the sandbox whitelist.
- **`resource_limit`**: The code exceeded memory or other resource limits (if supported by OS).

## Data Formats

### JSONL (`metrics.jsonl`)
The primary format for metrics. Each line is a JSON object representing one generation. This format is ideal for real-time streaming and programmatic analysis.

```json
{"generation": 0, "best_score_cheap": -15.2, "avg_score_cheap": -18.7, "n_generated": 50, "n_failed": 5, "failure_breakdown": {"syntax": 3, "timeout": 2}, "timestamp": "2024-01-15T10:30:00"}
```

### CSV (`metrics.csv`)
A flattened version of the metrics (excluding `failure_breakdown`) for easy import into Excel, Google Sheets, or Pandas.

## Interpretation Guide

### Healthy Evolution
- `best_score_full` should generally increase over generations.
- `n_deduped` should be non-zero, indicating that the system is successfully filtering redundant logic.
- `n_failed` should ideally be low (< 20% of `n_generated`).

### Troubleshooting
- **High `syntax` failures**: LLM might need a better prompt or a more capable model.
- **High `timeout` failures**: The heuristic logic might be too complex, or the sandbox timeout is too aggressive.
- **Stagnant `best_score`**: The population might have lost diversity. Consider increasing `population_size` or `num_islands`.
