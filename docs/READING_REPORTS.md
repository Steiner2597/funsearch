# Understanding Generated Reports

FunSearch-Lite automatically generates comprehensive reports after each experiment to help you analyze the search process and the quality of discovered heuristics.

## Report Formats

Reports are generated in two formats within the `artifacts/<run_id>/` directory:
- **`report.md`**: A Markdown version suitable for quick viewing in GitHub or VS Code.
- **`report.html`**: A rich HTML version with embedded visualizations, ideal for sharing or detailed analysis.

## Key Performance Indicators (KPIs)

The report highlights several critical metrics:

| KPI | Description |
|-----|-------------|
| **Best Score** | The highest fitness score achieved across all islands and generations. |
| **Unique Rate** | The percentage of generated candidates that were functionally unique (not skipped by deduplication). |
| **Generations to Best** | The generation number where the final best score was first discovered. |
| **Final Diversity** | The total number of unique candidates stored in the population at the end of the run. |

## Evolution Analysis

This section compares the starting best score (from the initial population or first generation) with the final best score.
- **Improvement**: The absolute difference between the final and starting best scores. A positive value indicates successful optimization.

## Per-Island Performance

FunSearch-Lite uses a multi-island model where multiple sub-populations evolve in parallel.
- **Island ID**: Unique identifier for each island.
- **Best Score**: The best score found within that specific island.
- **Avg Score**: The average score of candidates currently in that island.
- **Count**: The number of candidates residing in that island.

*Note: Significant differences between islands suggest that the multi-island model is successfully exploring different regions of the program space.*

## Visualizations

The report embeds several charts generated during the run:

### 1. Evolution Curve
Shows the trajectory of `Best Score`, `Average Score`, and `Top-K Average` over generations.
- **What to look for**: A steady upward trend in the best score. If the curve flattens early, the search may have converged prematurely.

### 2. Failure Distribution
A pie chart showing the breakdown of why candidates failed (Syntax, Runtime, Timeout, etc.).
- **What to look for**: If "Syntax" or "Import Blocked" is dominant, consider refining your LLM prompts.

## Deduplication Statistics

- **Total Candidates Attempted**: The total number of code samples generated by the LLM.
- **Duplicates Skipped**: How many of those samples were functionally identical to existing ones.
- **Deduplication Efficiency**: The percentage of search effort saved by avoiding redundant evaluations.

## Configuration Snapshot

The report includes the full YAML configuration used for the run, ensuring that your results are reproducible and the experimental setup is documented.
