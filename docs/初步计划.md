
我要构建一个【FunSearch 风格的 LLM 引导进化搜索系统】，用于学校 AI 课程的大型项目。
这个项目的目标不是单次生成代码，而是：
- 通过 LLM + 进化搜索
- 在明确可评估的任务上
- 自动发现和改进算法/策略
- 并能用实验数据证明其有效性

=====================
【项目总体目标】
=====================

项目名称暂定：FunSearch-Lite（Research-oriented, Reproducible）

系统应具备以下核心特征：
1. 以 Python 代码作为搜索空间（函数级别）
2. 通过 LLM 生成候选函数
3. 使用自动评估函数打分
4. 通过进化式循环不断改进
5. 保留多样性（而不是只爬一个局部最优）
6. 全过程可复现、可观测、可视化

该项目必须：
- 工程结构清晰
- 模块可独立测试
- 适合逐步扩展（vibe coding）
- 能作为“研究系统”而非脚本

=====================
【任务范围与约束】
=====================

- 编程语言：Python 3.10+
- 不允许在初期引入过重的分布式系统
- 初期以单机并行 / 简单并发为主
- 所有生成的代码都视为【不可信代码】，必须通过受控方式执行
- 评估函数必须是：
  - 确定性的（或可控随机）
  - 自动化的
  - 可产生数值 score

=====================
【系统架构要求】
=====================

请将系统拆分为清晰模块，并生成合理的仓库结构，包括但不限于：

1. funsearch_core/
   - 搜索循环（iteration loop）
   - population / islands 管理
   - selection / mutation / promotion 逻辑

2. llm/
   - LLM 调用抽象层
   - 支持 OpenAI-compatible 接口
   - 支持 prompt 模板管理
   - 支持失败重试与缓存

3. evaluator/
   - 任务定义（problem）
   - 自动评估函数
   - 支持多预算 / 多阶段评估（为创新点预留）

4. sandbox/
   - 用于执行 LLM 生成代码
   - 支持 timeout / 内存限制
   - 不允许网络访问
   - 可先实现“轻量级安全执行”

5. store/
   - 候选解存储
   - 保存 code / score / metadata / error
   - 支持去重（hash）

6. experiments/
   - 实验配置
   - seed 管理
   - 一键运行实验

7. ui/（后期）
   - 可视化 leaderboard
   - evolution 曲线
   - failure 统计

=====================
【核心数据结构要求】
=====================

系统中必须定义清晰的数据结构，例如：

Candidate:
- id
- code (string)
- score
- signature（用于多样性）
- parent_id
- generation
- runtime
- error_type

Run / Experiment:
- config
- seed
- metrics
- artifacts

=====================
【创新点预留（必须体现）】
=====================

系统设计时请为以下方向预留接口（不必一次实现）：

1. 多保真评估（cheap eval → expensive eval）
2. 多样性维护（signature clustering / novelty bonus）
3. 自动错误修复（compile-fix loop）
4. 成本感知搜索（减少无效评估）



=====================
【多模型厂商支持要求】
=====================

本项目必须支持接入多个主流大模型厂商的 API，
并通过统一抽象层进行管理与切换。

目标不是“写死某一个模型”，
而是构建一个【模型无关（Model-Agnostic）】的 FunSearch 系统。

=====================
【需要支持的模型类型（至少）】
=====================

系统需设计为可扩展，初期至少支持以下模型族：

1. OpenAI-compatible API
   - OpenAI
   - Azure OpenAI
   - DeepSeek（兼容 OpenAI schema）
   - Moonshot / Qwen / Yi（如兼容）
  
2. 国产模型（非完全兼容 OpenAI）
   - DeepSeek（chat / reasoner）
   - GLM（智谱）
   - 其他国产模型作为扩展点

不要求全部一次性可用，
但【架构上必须支持】。

=====================
【LLM 抽象层设计要求】
=====================

请设计一个统一的 LLM Provider 抽象接口，例如：

- BaseLLMProvider
  - generate(prompt, config) -> LLMResponse
  - supports_streaming
  - supports_function_call
  - max_context
  - cost_per_token (optional)

每个厂商模型通过 Adapter / Provider 方式接入，例如：
- OpenAIProvider
- DeepSeekProvider
- GLMProvider

系统其余部分【不得依赖具体厂商实现】。

=====================
【模型选择策略（创新点预留）】
=====================

系统中应支持以下模型调度方式：

1. 静态选择
   - 在 config 中指定模型

2. 动态选择（创新点）
   - cheap model → 用于大规模探索
   - strong model → 用于 top candidates 精修

例如：
- 使用 DeepSeek / GLM 进行大规模候选生成
- 使用 GPT-4 / 高端模型进行 mutation refinement

该策略需在架构中预留接口，即使初期不完全实现。

=====================
【Prompt & Response 规范】
=====================

由于不同模型输出风格不同：

- 所有生成的代码必须：
  - 通过统一 parser 解析
  - 严格提取 code block
  - 做基础 sanity check

请设计 prompt 模板，使其尽量：
- 与厂商无关
- 避免依赖特殊 token
- 支持 temperature / top-p 控制

=====================
【成本与实验记录（工程趋势）】
=====================

系统需在 LLM 调用时记录：

- 使用的模型
- 输入 token 数
- 输出 token 数
- 估计调用成本（如可得）

这些信息应存入 candidate metadata，
用于后续分析不同模型对 FunSearch 的影响。

=====================
【实验目标（报告用）】
=====================

系统应支持回答以下问题（即使是定性分析）：

- 不同模型对搜索质量的影响
- cheap model + strong model 组合是否优于单模型
- 模型能力 vs 搜索深度的 trade-off

这些将作为课程项目的关键实验结论。


=====================
【项目定位（请严格遵守）】
=====================

- 本项目是学校 AI 课程的大项目
- 目标不是工业级规模，而是：
  - 概念正确
  - 系统完整
  - 有清晰创新点
  - 有可复现实验
- 所有新增功能必须：
  - 可解释
  - 可评估
  - 能写进课程报告

=====================
【本阶段必须新增的核心能力】
=====================

请在现有架构中，设计并实现以下三类能力（允许最小实现）：

---------------------------------
1️⃣ 多保真评估（Multi-Fidelity Evaluation）
---------------------------------

目标：
- 降低搜索成本
- 提升搜索效率

要求：
- 设计至少两级评估：
  - cheap_eval：快速、低成本、低精度
  - full_eval：慢速、高精度
- Search Controller 应支持：
  - 先用 cheap_eval 过滤候选
  - 再对 Top-K 执行 full_eval
- 评估结果需记录在 candidate metadata 中

---------------------------------
2️⃣ 多模型协同搜索（Multi-Model FunSearch）
---------------------------------

目标：
- 体现模型能力与搜索效率的 trade-off

要求：
- 系统支持使用不同 LLM Provider 执行不同角色：
  - cheap model：大规模候选生成
  - strong model：Top-K 精修 / mutation
- 模型选择策略需：
  - 通过 config 指定
  - 在代码结构上可扩展
- 每个 candidate 必须记录：
  - 生成模型
  - 使用阶段（explore / refine）

---------------------------------
3️⃣ 搜索轨迹与系统行为可观测性
---------------------------------

目标：
- 展示“系统是如何进化的”

要求：
- 记录每一代：
  - best score
  - average score
  - candidate 数量
- 提供最小可视化方式：
  - 例如 matplotlib 输出 evolution 曲线
- 允许导出实验日志，用于课程报告作图

=====================
【工程与研究规范】
=====================

- 所有新增模块需：
  - 有清晰接口
  - 有 docstring
  - 有 TODO 标注未来扩展点
- 不允许一次性重构整个系统
- 请按“最小可运行闭环”原则推进
- 所有参数应通过 config 控制，而非硬编码

=====================
【创新点表达要求（非常重要）】
=====================

在设计与代码注释中，请显式体现以下研究视角：

- 为什么需要多保真评估？
- 为什么多模型协同优于单模型？
- 这些设计如何影响搜索行为与成本？

这些问题的答案应能直接用于课程报告。

